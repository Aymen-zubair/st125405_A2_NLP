{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f68c755d-7e2e-4bd2-ad47-f3b607d81f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.2.2+cu121\n",
      "TorchText version: 0.17.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"TorchText version:\", torchtext.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc1f38e6-d270-4783-af8a-ef814a90ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61f8b1c6-d39f-4106-a985-c156d13281c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4881c589-b53b-432d-8c8d-09500f8f3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f99cdfb9-92a4-43a9-92e6-9d911baff484",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a455849-5d4d-49c3-a10c-f6fa229d294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here i will be using the dataset from the hugging face dataset library that has 1 to 7 books of harry potter.\n",
    "#https://huggingface.co/datasets/WutYee/HarryPotter_books_1to7/viewer/default/train?p=813&row=81316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58d07594-b72b-407e-81fe-92d5ab7f6414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"WutYee/HarryPotter_books_1to7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb5510ca-1780-405a-895f-ae5109a45f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43425240-2ef2-4259-b5c5-ea4156b082f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 81349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 23118\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 23620\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0447e92-1856-45b5-bbf7-c396fcbbaab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81349, 1)\n"
     ]
    }
   ],
   "source": [
    "    print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aac1d3b-64f4-4aa7-a08d-2051b07f0ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING THE TEXT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d585faa4-08b0-4420-9c72-ad5c6abe3a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE NEED TO TOKENIZE THE DATA and remove the empty strings from the tokenized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea8d7b67-e9f3-4c49-b467-e3aa3862df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Tokenization function\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}\n",
    "\n",
    "# Apply tokenization to all splits of the dataset (train, validation, test)\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17548694-aa2a-44d5-a225-0a2c508f888d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cloak', ',', 'an', 'emerald', 'one', '.', 'her', 'black', 'hair', 'was', 'drawn', 'into', 'a', 'tight', 'bun', '.', 'she', 'looked']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][223]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ec35c5-1a02-47d4-8609-a9a8a7f3ee94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': [['hp', '1', '-', 'harry', 'potter', 'and', 'the'], ['sorcerer', \"'\", 's', 'stone'], ['harry', 'potter', 'and', 'the', 'sorcerer', \"'\", 's', 'stone'], [], [], ['harry', 'potter'], ['&'], ['the', 'sorcerer’s', 'stone'], [], [], ['by', 'j', '.', 'k', '.', 'rowling'], [], [], [], [], [], ['hp', '1', '-', 'harry', 'potter', 'and', 'the'], ['sorcerer', \"'\", 's', 'stone'], ['chapter', 'one'], []]}\n"
     ]
    }
   ],
   "source": [
    "# Check the first example after tokenization in the train split\n",
    "print(tokenized_dataset['train'][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f73db6fb-a4ce-40b1-9676-4b75774e8f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW TO PREPROCESS THE TOKENIZED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41046738-c910-44a0-83e4-2690fe0768fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5948399035a943dbb62b935c951396cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb1c8d8c9434da78cb73619adf67d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23118 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3df959f19e34d30af46d86133928f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282dc1249ccd4b07bba9d248a9cf9001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/690212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383eeeae6b364834b76d28e4be9b1c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/168502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ac7bda8e94948964be3a045c04510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/185948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['hp', 'harry', 'potter', 'and', 'the']}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tokens(examples):\n",
    "    # Clean tokens by removing unwanted characters and making them lowercase\n",
    "    cleaned_tokens = [\n",
    "        token.lower()  # Convert to lowercase\n",
    "        for example in examples['tokens']  # Iterate over individual tokens in the batch\n",
    "        for token in example  # Iterate over tokens in each example\n",
    "        if isinstance(token, str) and token.isalpha()  # Remove non-alphabetic tokens\n",
    "    ]\n",
    "    \n",
    "    return {'tokens': cleaned_tokens}  # Return cleaned tokens\n",
    "\n",
    "# Apply preprocessing with batched=True for efficiency\n",
    "cleaned_dataset = tokenized_dataset.map(preprocess_tokens, batched=True)\n",
    "\n",
    "# Remove entries where the 'tokens' list is empty\n",
    "cleaned_dataset = cleaned_dataset.filter(lambda x: len(x['tokens']) > 0)\n",
    "\n",
    "# Check if the dataset is now correctly processed\n",
    "print(cleaned_dataset['train'][:5])  # Print first 5 cleaned examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "da28999d-d331-4c3b-8000-adec0374ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['hp', 'harry', 'potter', 'and', 'the', 'sorcerer', 's', 'stone', 'harry', 'potter', 'and', 'the', 'sorcerer', 's', 'stone', 'harry', 'potter', 'the', 'stone', 'by', 'j', 'k', 'rowling', 'hp', 'harry', 'potter', 'and', 'the', 'sorcerer', 's', 'stone', 'chapter', 'one', 'the', 'boy', 'who', 'lived', 'm', 'r', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'hold', 'with', 'such', 'nonsense', 'mr', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills', 'he', 'was', 'a', 'big', 'beefy', 'man']}\n"
     ]
    }
   ],
   "source": [
    "# Checking THE PREPROCESSED DATA\n",
    "print(cleaned_dataset['train'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5edc640a-071a-4a7b-8588-10822b2dcd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning:\n",
      "{'tokens': [['hp', '1', '-', 'harry', 'potter', 'and', 'the'], ['sorcerer', \"'\", 's', 'stone'], ['harry', 'potter', 'and', 'the', 'sorcerer', \"'\", 's', 'stone'], [], [], ['harry', 'potter'], ['&'], ['the', 'sorcerer’s', 'stone'], [], []]}\n",
      "After cleaning:\n",
      "{'tokens': ['hp', 'harry', 'potter', 'and', 'the', 'sorcerer', 's', 'stone', 'harry', 'potter']}\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning\n",
    "print(\"Before cleaning:\")\n",
    "print(tokenized_dataset['train'][:10])\n",
    "\n",
    "# After cleaning\n",
    "print(\"After cleaning:\")\n",
    "print(cleaned_dataset['train'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36a2b764-c71f-4d10-a6b4-56f35b7e27ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 11368\n",
      "First 10 tokens in vocab: ['<unk>', '<eos>', '.', ',', 'the', '”', 'and', 'to', 'of', 'a', 'he', 'harry', 'was', 'said', 'his', 'in', 'it', 'you', '?', 'had']\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3)\n",
    "\n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bb5b498e-2e5e-4792-8313-905933b7d1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary: 11368\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of vocabulary: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7f54941e-1050-4434-b288-2df4b52c25a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 tokens in vocab: ['<unk>', '<eos>', '.', ',', 'the', '”', 'and', 'to', 'of', 'a', 'he', 'harry', 'was', 'said', 'his', 'in', 'it', 'you', '?', 'had']\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 20 tokens in vocab: {vocab.get_itos()[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cb6809ae-800d-4325-aafc-07a29b657d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we need to prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cedfe4ae-ce03-486f-91f6-2f92a808c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'] + ['<eos>']  # Add <eos> token at the end\n",
    "            token_indices = [vocab[token] for token in tokens]  # Convert tokens to indices\n",
    "            data.extend(token_indices)  # Add the indices to the data list\n",
    "    data = torch.LongTensor(data)  # Convert to tensor\n",
    "    num_batches = data.shape[0] // batch_size  # Calculate number of batches\n",
    "    data = data[:num_batches * batch_size]  # Truncate to ensure data is divisible by batch size\n",
    "    data = data.view(batch_size, -1)  # Reshape to [batch_size, seq_len]\n",
    "    return data  # Return the data in the required shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2ddcbddd-6e07-401b-ab13-fcec292a09b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 7660])\n"
     ]
    }
   ],
   "source": [
    "# Define batch size\n",
    "batch_size = 128\n",
    "\n",
    "# Prepare the data for training, validation, and testing\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data = get_data(tokenized_dataset['test'], vocab, batch_size)\n",
    "\n",
    "# Check the shape of the data\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b17d576d-f706-4680-9160-0222a4a0631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2352623a-cd45-4332-bcae-66ea80d2d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hid_dim    = hid_dim\n",
    "        self.emb_dim    = emb_dim\n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout    = nn.Dropout(dropout_rate)\n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hid_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim,\n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #We\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim,   \n",
    "                self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device)\n",
    "        return hidden, cell\n",
    "        \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden):\n",
    "        #src: [batch_size, seq len]\n",
    "        embedding = self.dropout(self.embedding(src)) #harry potter is\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        #ouput: [batch size, seq len, hid dim]\n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output = self.dropout(output)\n",
    "        prediction =self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9ed7a9e1-b8ce-4bb2-9e4e-768faf9ad42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 40,086,632 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)  # Size of vocabulary\n",
    "emb_dim = 1024           # Embedding dimension\n",
    "hid_dim = 1024           # Hidden dimension for LSTM\n",
    "num_layers = 2           # Number of LSTM layers\n",
    "dropout_rate = 0.65      # Dropout rate\n",
    "lr = 1e-3                # Learning rate\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMLanguageModel(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "\n",
    "# Optimizer and Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Model summary (Number of parameters)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "641c9c8a-abdd-46b6-b8b9-3df09a56937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    # Get source and target sequences from the batch\n",
    "    src = data[:, idx:idx+seq_len]                     \n",
    "    target = data[:, idx+1:idx+seq_len+1]                \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2daa6d5-6c4d-42ef-9c9c-e6cd8bd9bdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]  # Ensure data is multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device)  # Initialize hidden state for each epoch\n",
    "    \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ', leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden = model.detach_hidden(hidden)  # Detach hidden state to avoid backprop through the entire history\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx)  # Get source and target sequences\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        \n",
    "        batch_size = src.shape[0]  # Number of sequences in the batch\n",
    "        prediction, hidden = model(src, hidden)  # Get model predictions\n",
    "        \n",
    "        # Reshape predictions and target to calculate loss\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target = target.reshape(-1)\n",
    "        \n",
    "        loss = criterion(prediction, target)  # Compute loss\n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()  # Update parameters\n",
    "        \n",
    "        epoch_loss += loss.item() * seq_len  # Accumulate the loss\n",
    "    return epoch_loss / num_batches  # Return the average loss for the epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "441d30b9-e8c5-46aa-8ad0-6fa812411d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches - 1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size = src.shape[0]\n",
    "            \n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "            \n",
    "            loss = criterion(prediction, target)  # Compute loss\n",
    "            epoch_loss += loss.item() * seq_len  # Accumulate loss\n",
    "    return epoch_loss / num_batches  # Return the average loss for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bd64231c-9113-4a25-a097-b5c6500ddcf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 476.751\n",
      "\tValid Perplexity: 259.229\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 211.574\n",
      "\tValid Perplexity: 128.636\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 128.877\n",
      "\tValid Perplexity: 92.365\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 100.725\n",
      "\tValid Perplexity: 79.424\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 86.640\n",
      "\tValid Perplexity: 74.346\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 77.617\n",
      "\tValid Perplexity: 70.838\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 70.873\n",
      "\tValid Perplexity: 67.010\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 65.858\n",
      "\tValid Perplexity: 63.099\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 61.493\n",
      "\tValid Perplexity: 62.536\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 58.097\n",
      "\tValid Perplexity: 60.603\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 55.110\n",
      "\tValid Perplexity: 60.361\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 52.536\n",
      "\tValid Perplexity: 59.270\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 50.200\n",
      "\tValid Perplexity: 58.290\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 48.187\n",
      "\tValid Perplexity: 58.370\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 45.398\n",
      "\tValid Perplexity: 58.112\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 43.958\n",
      "\tValid Perplexity: 57.807\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 42.876\n",
      "\tValid Perplexity: 57.530\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 41.879\n",
      "\tValid Perplexity: 57.592\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 40.565\n",
      "\tValid Perplexity: 57.752\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.823\n",
      "\tValid Perplexity: 57.226\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.450\n",
      "\tValid Perplexity: 57.368\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 39.008\n",
      "\tValid Perplexity: 56.996\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.765\n",
      "\tValid Perplexity: 56.909\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.714\n",
      "\tValid Perplexity: 56.808\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.538\n",
      "\tValid Perplexity: 56.997\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.350\n",
      "\tValid Perplexity: 56.738\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.239\n",
      "\tValid Perplexity: 56.665\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.171\n",
      "\tValid Perplexity: 56.694\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.071\n",
      "\tValid Perplexity: 56.627\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 38.068\n",
      "\tValid Perplexity: 56.723\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.976\n",
      "\tValid Perplexity: 56.649\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.918\n",
      "\tValid Perplexity: 56.627\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.935\n",
      "\tValid Perplexity: 56.622\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.909\n",
      "\tValid Perplexity: 56.627\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.908\n",
      "\tValid Perplexity: 56.623\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.922\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.903\n",
      "\tValid Perplexity: 56.617\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.921\n",
      "\tValid Perplexity: 56.617\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.911\n",
      "\tValid Perplexity: 56.617\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.945\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.927\n",
      "\tValid Perplexity: 56.617\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.905\n",
      "\tValid Perplexity: 56.617\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.962\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.959\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.933\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.862\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.936\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.940\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.879\n",
      "\tValid Perplexity: 56.618\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Perplexity: 37.905\n",
      "\tValid Perplexity: 56.618\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50  # Number of epochs\n",
    "seq_len = 50    # Sequence length (decoding length)\n",
    "clip = 0.25     # Gradient clipping\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')  # Initialize the best valid loss as a large value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}\")  # Display the current epoch number\n",
    "    # Train the model for one epoch\n",
    "    train_loss = train(model, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    \n",
    "    # Evaluate the model on validation set\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, seq_len, device)\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    lr_scheduler.step(valid_loss)\n",
    "    \n",
    "    # Save the model if validation loss improves\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-lstm_lm.pt')\n",
    "\n",
    "    # Print train and validation perplexity for the current epoch\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6102082c-18ea-461a-8aae-37cbaef40e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perplexity: 70.518\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-lstm_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3cc89de3-0394-4d59-847b-23a70d17a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    # Set seed for reproducibility\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    tokens = tokenizer(prompt)  # Tokenize the input prompt\n",
    "    indices = [vocab[t] for t in tokens]  # Convert tokens to indices\n",
    "    batch_size = 1  # Single example batch size\n",
    "    hidden = model.init_hidden(batch_size, device)  # Initialize hidden state\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation during generation\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)  # Input sequence for the model\n",
    "            prediction, hidden = model(src, hidden)  # Get model predictions\n",
    "            \n",
    "            # Softmax on the last token's predictions to get probabilities\n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            # If the prediction is <unk>, sample again\n",
    "            while prediction == vocab['<unk>']:\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            # Stop if <eos> token is predicted\n",
    "            if prediction == vocab['<eos>']:\n",
    "                break\n",
    "            \n",
    "            indices.append(prediction)  # Add the predicted token to the sequence\n",
    "        \n",
    "    # Convert indices back to tokens\n",
    "    itos = vocab.get_itos()  # Get the reverse vocabulary (index to token mapping)\n",
    "    generated_tokens = [itos[i] for i in indices]  # Map indices back to tokens\n",
    "    return generated_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d19f6b4b-99a1-4fd4-97d7-c53f8279efcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<eos>', '.', ',', 'the', '”', 'and', 'to', 'of', 'a', 'he', 'harry', 'was', 'said', 'his', 'in', 'it', 'you', '?', 'had', 'at', 'that', 'i', '!', 'on', 'as', 'him', 'with', '—', 'they', 'ron', \"'\", 'for', 'her', 'but', '\\x91', 'hermione', 'up', 'out', 'she', 'be', 'were', 'not', 'all', 'them', 'from', 'have', 'what', 'back', 'into', 'been', 'there', 'this', 'me', 'is', 'their', 'so', 'one', 'who', 'about', 'could', 'then', 'down', 's', 'now', 'if', 'over', 'we', 'around', 'looked', '“i', 'like', 'very', 'just', 'professor', 'when', 'an', 'got', '—”', 'know', 'dumbledore', 'by', 'hagrid', 'do', 'would', 'your', 'off', 'again', 'no', 'see', 'though', 'more', 'are', 'did', 'my', 'potter', 'get', 'looking', 'weasley', 't']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.get_itos()[:100])  # Check if common words exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "552da567-1bf6-4346-8f25-2326b5be641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.3\n",
      "Generated Text: harry looked at harry .\n",
      "--------------------------------------------------\n",
      "Temperature: 0.5\n",
      "Generated Text: harry looked at harry .\n",
      "--------------------------------------------------\n",
      "Temperature: 0.7\n",
      "Generated Text: harry looked at it .\n",
      "--------------------------------------------------\n",
      "Temperature: 0.9\n",
      "Generated Text: harry looked up at\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "prompt = \"harry looked\"\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "# Experimenting with a broader range of temperatures\n",
    "temperatures = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed)\n",
    "    sentence = \" \".join(generation)\n",
    "\n",
    "    print(f\"Temperature: {temperature}\\nGenerated Text: {sentence}\\n\" + \"-\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5e6d1ad-6d7a-4582-b71c-2827b8ffeb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The language model is built using an LSTM (Long Short-Term Memory), which helps it understand and generate text. \n",
    "#It starts by converting words into numerical embeddings, then passes them through the LSTM layers to learn patterns in the text.\n",
    "#Finally, a fully connected layer predicts the next word.\n",
    "#During training, the model learns to predict the next word and adjusts its parameters when it makes mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66710b76-003e-4a5c-98c6-044603fd220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the vocabulary to a file\n",
    "torch.save(vocab, \"vocab.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
